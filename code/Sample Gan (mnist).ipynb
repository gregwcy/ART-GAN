{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6474d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94dbe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f627ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build generator\n",
    "\n",
    "def build_generator():\n",
    "    \n",
    "    noise_shape = (100,)   #1D array of size 100 (latent vector/ noise)\n",
    "    alpha = 0.2\n",
    "    momentum = 0.8\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(256, input_shape=noise_shape))\n",
    "    model.add(LeakyReLU(alpha = alpha))\n",
    "    model.add(BatchNormalization(momentum = momentum))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = alpha))\n",
    "    model.add(BatchNormalization(momentum = momentum))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha = alpha))\n",
    "    model.add(BatchNormalization(momentum = momentum))\n",
    "    \n",
    "    model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "    model.add(Reshape(img_shape))\n",
    "    \n",
    "    noise = Input(shape = noise_shape)\n",
    "    img = model(noise)\n",
    "    \n",
    "    return Model(noise, img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc03c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build discriminator\n",
    "\n",
    "def build_discriminator():\n",
    "    \n",
    "    alpha = 0.2\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Flatten(input_shape=img_shape))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha = alpha))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha = alpha))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "    \n",
    "    return Model(img,validity)       # validity is whether image is real or fake\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9110adf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training part\n",
    "\n",
    "def train(epochs, batch_size = 128, save_interval = 500):\n",
    "    \n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "    \n",
    "    X_train = (X_train.astype(np.float32)) / 255\n",
    "    \n",
    "    X_train = np.expand_dims(X_train, axis = 3)   # adding 3rd axis so its 28 x 28 x 1\n",
    "    \n",
    "    half_batch = int(batch_size/2)                # mixing half real and half fake\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Select sample of real images\n",
    "        idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "        imgs = X_train[idx]\n",
    "        \n",
    "        noise = np.random.normal(0,1,(half_batch, 100))      # 64 rows and 100 columns\n",
    "        \n",
    "        # Generate images\n",
    "        gen_imgs = generator.predict(noise)                 # generator produces fake images\n",
    "        \n",
    "        \n",
    "        # Train the discriminator on real and fake images, seperately\n",
    "        # Research shows seperate training is more effective\n",
    "        d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "        \n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train generator within epoch\n",
    "        \n",
    "        noise = np.random.normal(0,1,(half_batch, 100))\n",
    "        \n",
    "        \n",
    "        valid_y = np.array([1] * half_batch)              # Creates an array of 1s in column format, size = batch_size\n",
    "        \n",
    "        # We fool the discriminator by labelling fakes as real\n",
    "        g_loss = combined.train_on_batch(noise, valid_y)\n",
    "        \n",
    "        \n",
    "        print('%d [D loss: %f, acc.:%.2f%%] [G loss:%f]' %(epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "        \n",
    "        if epoch % save_interval == 0:\n",
    "            save_imgs(epoch)\n",
    "\n",
    "def save_imgs(epoch):\n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r*c, 100))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    # rescale images 0-1\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "    \n",
    "    fig, axs = plt.subplots(r, c)\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap = 'gray')\n",
    "            axs[i,j].axis('off')\n",
    "            cnt+=1\n",
    "    fig.savefig(\"./mnistimages/mnist_%d.png\" % epoch)\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a58f10f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.764120, acc.:40.62%] [G loss:0.286336]\n",
      "1 [D loss: 0.310019, acc.:93.75%] [G loss:0.260020]\n",
      "2 [D loss: 0.251927, acc.:90.62%] [G loss:0.459756]\n",
      "3 [D loss: 0.446438, acc.:65.62%] [G loss:0.959749]\n",
      "4 [D loss: 0.548930, acc.:62.50%] [G loss:1.491198]\n",
      "5 [D loss: 0.505708, acc.:78.12%] [G loss:2.853218]\n",
      "6 [D loss: 0.292758, acc.:78.12%] [G loss:3.574903]\n",
      "7 [D loss: 0.040649, acc.:100.00%] [G loss:4.518363]\n",
      "8 [D loss: 0.035182, acc.:100.00%] [G loss:6.182604]\n",
      "9 [D loss: 0.077513, acc.:100.00%] [G loss:7.016088]\n",
      "10 [D loss: 0.098104, acc.:96.88%] [G loss:7.601319]\n",
      "11 [D loss: 0.056585, acc.:100.00%] [G loss:7.864446]\n",
      "12 [D loss: 0.030868, acc.:100.00%] [G loss:8.879819]\n",
      "13 [D loss: 0.015707, acc.:100.00%] [G loss:8.442834]\n",
      "14 [D loss: 0.007806, acc.:100.00%] [G loss:8.533834]\n",
      "15 [D loss: 0.015064, acc.:100.00%] [G loss:8.687078]\n",
      "16 [D loss: 0.006832, acc.:100.00%] [G loss:9.208979]\n",
      "17 [D loss: 0.007512, acc.:100.00%] [G loss:8.908137]\n",
      "18 [D loss: 0.039400, acc.:100.00%] [G loss:8.777124]\n",
      "19 [D loss: 0.051778, acc.:96.88%] [G loss:9.308640]\n",
      "20 [D loss: 0.005723, acc.:100.00%] [G loss:9.387585]\n",
      "21 [D loss: 0.002860, acc.:100.00%] [G loss:9.730553]\n",
      "22 [D loss: 0.008614, acc.:100.00%] [G loss:10.303107]\n",
      "23 [D loss: 0.004579, acc.:100.00%] [G loss:10.538624]\n",
      "24 [D loss: 0.001552, acc.:100.00%] [G loss:10.693518]\n",
      "25 [D loss: 0.003374, acc.:100.00%] [G loss:10.975303]\n",
      "26 [D loss: 0.005609, acc.:100.00%] [G loss:10.198834]\n",
      "27 [D loss: 0.009235, acc.:100.00%] [G loss:9.930239]\n",
      "28 [D loss: 0.001363, acc.:100.00%] [G loss:10.315257]\n",
      "29 [D loss: 0.004702, acc.:100.00%] [G loss:9.911359]\n",
      "30 [D loss: 0.001613, acc.:100.00%] [G loss:10.501665]\n",
      "31 [D loss: 0.004828, acc.:100.00%] [G loss:9.065509]\n",
      "32 [D loss: 0.246255, acc.:93.75%] [G loss:9.241752]\n",
      "33 [D loss: 0.027517, acc.:96.88%] [G loss:9.960287]\n",
      "34 [D loss: 0.093722, acc.:96.88%] [G loss:9.414283]\n",
      "35 [D loss: 0.023357, acc.:100.00%] [G loss:11.261644]\n",
      "36 [D loss: 0.074594, acc.:100.00%] [G loss:10.791197]\n",
      "37 [D loss: 0.038142, acc.:100.00%] [G loss:10.454348]\n",
      "38 [D loss: 0.029237, acc.:100.00%] [G loss:10.021088]\n",
      "39 [D loss: 0.069318, acc.:96.88%] [G loss:9.280890]\n",
      "40 [D loss: 0.003621, acc.:100.00%] [G loss:9.464396]\n",
      "41 [D loss: 0.002594, acc.:100.00%] [G loss:8.471327]\n",
      "42 [D loss: 0.121683, acc.:96.88%] [G loss:7.834687]\n",
      "43 [D loss: 0.031537, acc.:100.00%] [G loss:8.884540]\n",
      "44 [D loss: 0.137952, acc.:96.88%] [G loss:8.820677]\n",
      "45 [D loss: 0.014579, acc.:100.00%] [G loss:10.585404]\n",
      "46 [D loss: 0.043007, acc.:96.88%] [G loss:10.529198]\n",
      "47 [D loss: 0.053683, acc.:96.88%] [G loss:11.082783]\n",
      "48 [D loss: 0.048945, acc.:100.00%] [G loss:12.356979]\n",
      "49 [D loss: 0.070577, acc.:96.88%] [G loss:10.812950]\n",
      "50 [D loss: 0.029250, acc.:100.00%] [G loss:9.541786]\n",
      "51 [D loss: 0.012495, acc.:100.00%] [G loss:11.244756]\n",
      "52 [D loss: 0.039309, acc.:96.88%] [G loss:10.289838]\n",
      "53 [D loss: 0.109526, acc.:93.75%] [G loss:10.880396]\n",
      "54 [D loss: 0.008222, acc.:100.00%] [G loss:11.115961]\n",
      "55 [D loss: 0.146704, acc.:93.75%] [G loss:12.371296]\n",
      "56 [D loss: 0.057971, acc.:100.00%] [G loss:11.355194]\n",
      "57 [D loss: 0.234639, acc.:96.88%] [G loss:11.939980]\n",
      "58 [D loss: 0.026711, acc.:100.00%] [G loss:11.744859]\n",
      "59 [D loss: 0.042051, acc.:100.00%] [G loss:12.137270]\n",
      "60 [D loss: 0.018138, acc.:100.00%] [G loss:12.719891]\n",
      "61 [D loss: 0.024668, acc.:100.00%] [G loss:12.745851]\n",
      "62 [D loss: 0.195245, acc.:96.88%] [G loss:11.981986]\n",
      "63 [D loss: 0.021126, acc.:100.00%] [G loss:11.609106]\n",
      "64 [D loss: 0.004389, acc.:100.00%] [G loss:11.537410]\n",
      "65 [D loss: 0.143167, acc.:96.88%] [G loss:12.389132]\n",
      "66 [D loss: 0.024602, acc.:100.00%] [G loss:11.885415]\n",
      "67 [D loss: 0.057637, acc.:100.00%] [G loss:11.525387]\n",
      "68 [D loss: 0.051021, acc.:100.00%] [G loss:12.087158]\n",
      "69 [D loss: 0.139115, acc.:93.75%] [G loss:10.327679]\n",
      "70 [D loss: 0.019601, acc.:100.00%] [G loss:11.793976]\n",
      "71 [D loss: 0.174819, acc.:90.62%] [G loss:11.550909]\n",
      "72 [D loss: 0.036187, acc.:96.88%] [G loss:12.813525]\n",
      "73 [D loss: 0.039699, acc.:100.00%] [G loss:12.801675]\n",
      "74 [D loss: 0.021419, acc.:100.00%] [G loss:13.594470]\n",
      "75 [D loss: 0.057729, acc.:100.00%] [G loss:12.223998]\n",
      "76 [D loss: 0.036150, acc.:100.00%] [G loss:11.935004]\n",
      "77 [D loss: 0.120541, acc.:93.75%] [G loss:11.563446]\n",
      "78 [D loss: 0.116154, acc.:96.88%] [G loss:11.764204]\n",
      "79 [D loss: 0.016745, acc.:100.00%] [G loss:15.717329]\n",
      "80 [D loss: 0.177337, acc.:93.75%] [G loss:14.884016]\n",
      "81 [D loss: 0.106362, acc.:96.88%] [G loss:16.237944]\n",
      "82 [D loss: 0.076565, acc.:93.75%] [G loss:15.802218]\n",
      "83 [D loss: 0.036271, acc.:100.00%] [G loss:13.506926]\n",
      "84 [D loss: 0.029134, acc.:100.00%] [G loss:13.243145]\n",
      "85 [D loss: 0.008731, acc.:100.00%] [G loss:11.586760]\n",
      "86 [D loss: 0.550455, acc.:90.62%] [G loss:13.931870]\n",
      "87 [D loss: 0.026352, acc.:100.00%] [G loss:14.848866]\n",
      "88 [D loss: 0.264453, acc.:96.88%] [G loss:16.387066]\n",
      "89 [D loss: 0.099229, acc.:100.00%] [G loss:14.716773]\n",
      "90 [D loss: 0.118150, acc.:96.88%] [G loss:13.043932]\n",
      "91 [D loss: 0.048777, acc.:100.00%] [G loss:12.774134]\n",
      "92 [D loss: 0.055740, acc.:96.88%] [G loss:11.038527]\n",
      "93 [D loss: 0.105393, acc.:96.88%] [G loss:9.420248]\n",
      "94 [D loss: 0.086222, acc.:96.88%] [G loss:12.486316]\n",
      "95 [D loss: 0.067148, acc.:96.88%] [G loss:12.875122]\n",
      "96 [D loss: 0.040120, acc.:100.00%] [G loss:12.789127]\n",
      "97 [D loss: 0.106688, acc.:93.75%] [G loss:14.402111]\n",
      "98 [D loss: 0.027402, acc.:100.00%] [G loss:17.228642]\n",
      "99 [D loss: 0.052187, acc.:100.00%] [G loss:13.432217]\n",
      "100 [D loss: 0.280231, acc.:93.75%] [G loss:14.998249]\n",
      "101 [D loss: 0.317201, acc.:87.50%] [G loss:13.883000]\n",
      "102 [D loss: 0.013108, acc.:100.00%] [G loss:16.638845]\n",
      "103 [D loss: 0.199481, acc.:93.75%] [G loss:18.548531]\n",
      "104 [D loss: 0.047674, acc.:96.88%] [G loss:18.988960]\n",
      "105 [D loss: 0.114059, acc.:93.75%] [G loss:19.539856]\n",
      "106 [D loss: 0.066597, acc.:96.88%] [G loss:21.273834]\n",
      "107 [D loss: 0.099413, acc.:93.75%] [G loss:16.656906]\n",
      "108 [D loss: 0.010419, acc.:100.00%] [G loss:13.108828]\n",
      "109 [D loss: 0.028906, acc.:100.00%] [G loss:10.701162]\n",
      "110 [D loss: 0.166281, acc.:93.75%] [G loss:11.800147]\n",
      "111 [D loss: 0.059223, acc.:96.88%] [G loss:12.595977]\n",
      "112 [D loss: 0.137474, acc.:96.88%] [G loss:14.977808]\n",
      "113 [D loss: 0.044287, acc.:96.88%] [G loss:16.883301]\n",
      "114 [D loss: 0.051717, acc.:100.00%] [G loss:19.175215]\n",
      "115 [D loss: 0.039616, acc.:100.00%] [G loss:18.119511]\n",
      "116 [D loss: 0.016980, acc.:100.00%] [G loss:17.796528]\n",
      "117 [D loss: 0.015729, acc.:100.00%] [G loss:14.038671]\n",
      "118 [D loss: 0.040639, acc.:100.00%] [G loss:13.503010]\n",
      "119 [D loss: 0.334696, acc.:90.62%] [G loss:14.066103]\n",
      "120 [D loss: 0.026773, acc.:100.00%] [G loss:14.127233]\n",
      "121 [D loss: 0.172733, acc.:93.75%] [G loss:15.708453]\n",
      "122 [D loss: 0.008374, acc.:100.00%] [G loss:19.417282]\n",
      "123 [D loss: 0.068599, acc.:96.88%] [G loss:20.880133]\n",
      "124 [D loss: 0.096354, acc.:100.00%] [G loss:19.052258]\n",
      "125 [D loss: 0.035151, acc.:100.00%] [G loss:16.271517]\n",
      "126 [D loss: 0.025386, acc.:100.00%] [G loss:14.181745]\n",
      "127 [D loss: 0.158806, acc.:96.88%] [G loss:15.254368]\n",
      "128 [D loss: 0.126986, acc.:96.88%] [G loss:17.085186]\n",
      "129 [D loss: 0.001453, acc.:100.00%] [G loss:18.874844]\n",
      "130 [D loss: 0.272235, acc.:96.88%] [G loss:23.688606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 [D loss: 0.064826, acc.:96.88%] [G loss:23.188292]\n",
      "132 [D loss: 0.071553, acc.:96.88%] [G loss:22.458523]\n",
      "133 [D loss: 0.069980, acc.:96.88%] [G loss:21.644001]\n",
      "134 [D loss: 0.335399, acc.:96.88%] [G loss:19.932758]\n",
      "135 [D loss: 0.013467, acc.:100.00%] [G loss:18.088310]\n",
      "136 [D loss: 0.027143, acc.:100.00%] [G loss:15.794668]\n",
      "137 [D loss: 0.054897, acc.:96.88%] [G loss:14.218191]\n",
      "138 [D loss: 0.006074, acc.:100.00%] [G loss:16.210281]\n",
      "139 [D loss: 0.004360, acc.:100.00%] [G loss:15.932207]\n",
      "140 [D loss: 0.009859, acc.:100.00%] [G loss:15.223002]\n",
      "141 [D loss: 0.298764, acc.:96.88%] [G loss:15.594488]\n",
      "142 [D loss: 0.004166, acc.:100.00%] [G loss:15.759548]\n",
      "143 [D loss: 0.194297, acc.:96.88%] [G loss:18.913671]\n",
      "144 [D loss: 0.051100, acc.:100.00%] [G loss:19.781002]\n",
      "145 [D loss: 0.094354, acc.:96.88%] [G loss:20.810982]\n",
      "146 [D loss: 0.107867, acc.:93.75%] [G loss:19.709187]\n",
      "147 [D loss: 0.024908, acc.:100.00%] [G loss:21.935736]\n",
      "148 [D loss: 0.037826, acc.:100.00%] [G loss:18.790073]\n",
      "149 [D loss: 0.084195, acc.:96.88%] [G loss:17.257734]\n",
      "150 [D loss: 0.049569, acc.:96.88%] [G loss:20.690418]\n",
      "151 [D loss: 0.011332, acc.:100.00%] [G loss:23.573814]\n",
      "152 [D loss: 0.005491, acc.:100.00%] [G loss:22.930305]\n",
      "153 [D loss: 0.022256, acc.:100.00%] [G loss:26.200558]\n",
      "154 [D loss: 0.047116, acc.:100.00%] [G loss:26.186356]\n",
      "155 [D loss: 0.019001, acc.:100.00%] [G loss:25.120754]\n",
      "156 [D loss: 0.035189, acc.:100.00%] [G loss:24.796835]\n",
      "157 [D loss: 0.017760, acc.:100.00%] [G loss:22.316841]\n",
      "158 [D loss: 0.051633, acc.:96.88%] [G loss:19.156063]\n",
      "159 [D loss: 0.018878, acc.:100.00%] [G loss:17.430599]\n",
      "160 [D loss: 0.278370, acc.:90.62%] [G loss:18.093683]\n",
      "161 [D loss: 0.002730, acc.:100.00%] [G loss:23.166801]\n",
      "162 [D loss: 0.028627, acc.:100.00%] [G loss:32.233994]\n",
      "163 [D loss: 0.035205, acc.:100.00%] [G loss:29.141687]\n",
      "164 [D loss: 0.543017, acc.:87.50%] [G loss:31.216084]\n",
      "165 [D loss: 0.088966, acc.:96.88%] [G loss:32.714512]\n",
      "166 [D loss: 0.051480, acc.:100.00%] [G loss:32.681747]\n",
      "167 [D loss: 0.023581, acc.:100.00%] [G loss:27.031523]\n",
      "168 [D loss: 0.024737, acc.:100.00%] [G loss:28.208050]\n",
      "169 [D loss: 0.006859, acc.:100.00%] [G loss:24.995872]\n",
      "170 [D loss: 0.076651, acc.:96.88%] [G loss:25.361755]\n",
      "171 [D loss: 0.219422, acc.:96.88%] [G loss:27.454243]\n",
      "172 [D loss: 0.023951, acc.:100.00%] [G loss:26.563347]\n",
      "173 [D loss: 0.015121, acc.:100.00%] [G loss:26.177135]\n",
      "174 [D loss: 0.013154, acc.:100.00%] [G loss:26.212105]\n",
      "175 [D loss: 0.023511, acc.:100.00%] [G loss:24.214737]\n",
      "176 [D loss: 0.038870, acc.:96.88%] [G loss:21.831104]\n",
      "177 [D loss: 0.259334, acc.:90.62%] [G loss:19.958382]\n",
      "178 [D loss: 0.045211, acc.:100.00%] [G loss:20.071045]\n",
      "179 [D loss: 0.039961, acc.:100.00%] [G loss:20.403225]\n",
      "180 [D loss: 0.040270, acc.:100.00%] [G loss:18.317371]\n",
      "181 [D loss: 0.085061, acc.:93.75%] [G loss:16.117094]\n",
      "182 [D loss: 0.378140, acc.:90.62%] [G loss:18.368649]\n",
      "183 [D loss: 0.008567, acc.:100.00%] [G loss:21.973545]\n",
      "184 [D loss: 0.063490, acc.:96.88%] [G loss:24.230209]\n",
      "185 [D loss: 0.101197, acc.:93.75%] [G loss:24.144007]\n",
      "186 [D loss: 0.030905, acc.:100.00%] [G loss:20.814302]\n",
      "187 [D loss: 0.066423, acc.:96.88%] [G loss:21.708424]\n",
      "188 [D loss: 0.289508, acc.:93.75%] [G loss:21.808563]\n",
      "189 [D loss: 0.012823, acc.:100.00%] [G loss:23.054022]\n",
      "190 [D loss: 0.031611, acc.:100.00%] [G loss:22.502298]\n",
      "191 [D loss: 0.028134, acc.:100.00%] [G loss:28.667574]\n",
      "192 [D loss: 0.038301, acc.:100.00%] [G loss:23.348263]\n",
      "193 [D loss: 0.032579, acc.:100.00%] [G loss:23.377569]\n",
      "194 [D loss: 0.025143, acc.:100.00%] [G loss:20.869314]\n",
      "195 [D loss: 0.024542, acc.:100.00%] [G loss:17.725204]\n",
      "196 [D loss: 0.533379, acc.:90.62%] [G loss:16.724632]\n",
      "197 [D loss: 0.035575, acc.:100.00%] [G loss:19.936312]\n",
      "198 [D loss: 0.006445, acc.:100.00%] [G loss:24.672901]\n",
      "199 [D loss: 0.061608, acc.:96.88%] [G loss:25.857895]\n",
      "200 [D loss: 0.210404, acc.:93.75%] [G loss:25.173817]\n",
      "201 [D loss: 0.018372, acc.:100.00%] [G loss:26.071331]\n",
      "202 [D loss: 0.044061, acc.:100.00%] [G loss:23.461052]\n",
      "203 [D loss: 0.038067, acc.:100.00%] [G loss:24.354513]\n",
      "204 [D loss: 0.018645, acc.:100.00%] [G loss:23.013128]\n",
      "205 [D loss: 0.087348, acc.:96.88%] [G loss:22.067532]\n",
      "206 [D loss: 0.028008, acc.:100.00%] [G loss:21.907593]\n",
      "207 [D loss: 0.014618, acc.:100.00%] [G loss:20.440426]\n",
      "208 [D loss: 0.010885, acc.:100.00%] [G loss:18.657696]\n",
      "209 [D loss: 0.053455, acc.:100.00%] [G loss:21.185318]\n",
      "210 [D loss: 0.008472, acc.:100.00%] [G loss:20.354231]\n",
      "211 [D loss: 0.018383, acc.:100.00%] [G loss:23.546921]\n",
      "212 [D loss: 0.073591, acc.:96.88%] [G loss:18.338650]\n",
      "213 [D loss: 0.017532, acc.:100.00%] [G loss:21.107067]\n",
      "214 [D loss: 0.026971, acc.:100.00%] [G loss:17.746876]\n",
      "215 [D loss: 0.009582, acc.:100.00%] [G loss:16.460268]\n",
      "216 [D loss: 0.018463, acc.:100.00%] [G loss:15.928349]\n",
      "217 [D loss: 0.085571, acc.:93.75%] [G loss:13.800499]\n",
      "218 [D loss: 0.056703, acc.:96.88%] [G loss:11.820646]\n",
      "219 [D loss: 0.023601, acc.:100.00%] [G loss:12.594881]\n",
      "220 [D loss: 0.165990, acc.:96.88%] [G loss:15.719344]\n",
      "221 [D loss: 0.088787, acc.:96.88%] [G loss:21.510273]\n",
      "222 [D loss: 0.181857, acc.:96.88%] [G loss:20.624081]\n",
      "223 [D loss: 0.248588, acc.:87.50%] [G loss:14.361158]\n",
      "224 [D loss: 0.331976, acc.:90.62%] [G loss:13.901938]\n",
      "225 [D loss: 0.025730, acc.:100.00%] [G loss:19.759949]\n",
      "226 [D loss: 0.066555, acc.:96.88%] [G loss:19.110075]\n",
      "227 [D loss: 0.040024, acc.:100.00%] [G loss:23.540270]\n",
      "228 [D loss: 0.047243, acc.:100.00%] [G loss:22.089695]\n",
      "229 [D loss: 0.022976, acc.:100.00%] [G loss:15.564907]\n",
      "230 [D loss: 0.063269, acc.:96.88%] [G loss:17.775681]\n",
      "231 [D loss: 0.085905, acc.:96.88%] [G loss:19.096943]\n",
      "232 [D loss: 0.028197, acc.:100.00%] [G loss:20.123024]\n",
      "233 [D loss: 0.063942, acc.:96.88%] [G loss:19.292982]\n",
      "234 [D loss: 0.139550, acc.:96.88%] [G loss:22.266476]\n",
      "235 [D loss: 0.043852, acc.:100.00%] [G loss:19.297676]\n",
      "236 [D loss: 0.025305, acc.:100.00%] [G loss:20.625330]\n",
      "237 [D loss: 0.018489, acc.:100.00%] [G loss:17.060768]\n",
      "238 [D loss: 0.024107, acc.:100.00%] [G loss:15.232330]\n",
      "239 [D loss: 0.029155, acc.:100.00%] [G loss:17.990286]\n",
      "240 [D loss: 0.021218, acc.:100.00%] [G loss:15.896639]\n",
      "241 [D loss: 0.026723, acc.:100.00%] [G loss:18.157249]\n",
      "242 [D loss: 0.034968, acc.:100.00%] [G loss:20.602303]\n",
      "243 [D loss: 0.056332, acc.:100.00%] [G loss:17.088825]\n",
      "244 [D loss: 0.256167, acc.:93.75%] [G loss:19.081493]\n",
      "245 [D loss: 0.169399, acc.:93.75%] [G loss:25.909763]\n",
      "246 [D loss: 0.127072, acc.:96.88%] [G loss:25.426136]\n",
      "247 [D loss: 0.107478, acc.:93.75%] [G loss:21.176304]\n",
      "248 [D loss: 0.016503, acc.:100.00%] [G loss:17.634583]\n",
      "249 [D loss: 1.059087, acc.:87.50%] [G loss:23.371517]\n",
      "250 [D loss: 0.029052, acc.:100.00%] [G loss:34.786621]\n",
      "251 [D loss: 0.218940, acc.:90.62%] [G loss:34.923500]\n",
      "252 [D loss: 0.253990, acc.:90.62%] [G loss:29.478542]\n",
      "253 [D loss: 0.106437, acc.:96.88%] [G loss:21.306370]\n",
      "254 [D loss: 0.132151, acc.:93.75%] [G loss:25.215126]\n",
      "255 [D loss: 0.173926, acc.:96.88%] [G loss:31.107075]\n",
      "256 [D loss: 0.057203, acc.:96.88%] [G loss:39.160652]\n",
      "257 [D loss: 0.069425, acc.:96.88%] [G loss:36.291195]\n",
      "258 [D loss: 0.093807, acc.:100.00%] [G loss:35.692207]\n",
      "259 [D loss: 0.045249, acc.:100.00%] [G loss:31.554585]\n",
      "260 [D loss: 0.023193, acc.:100.00%] [G loss:24.438919]\n",
      "261 [D loss: 0.196054, acc.:90.62%] [G loss:31.811935]\n",
      "262 [D loss: 0.020444, acc.:100.00%] [G loss:31.152855]\n",
      "263 [D loss: 0.032802, acc.:100.00%] [G loss:35.727707]\n",
      "264 [D loss: 0.076120, acc.:100.00%] [G loss:30.346130]\n",
      "265 [D loss: 0.210150, acc.:93.75%] [G loss:26.797033]\n",
      "266 [D loss: 0.591301, acc.:84.38%] [G loss:23.598326]\n",
      "267 [D loss: 0.013902, acc.:100.00%] [G loss:31.975424]\n",
      "268 [D loss: 0.025525, acc.:100.00%] [G loss:30.180933]\n",
      "269 [D loss: 0.109353, acc.:93.75%] [G loss:28.679817]\n",
      "270 [D loss: 0.388260, acc.:90.62%] [G loss:25.448666]\n",
      "271 [D loss: 0.032881, acc.:100.00%] [G loss:26.598888]\n",
      "272 [D loss: 0.034505, acc.:100.00%] [G loss:19.913792]\n",
      "273 [D loss: 0.053676, acc.:96.88%] [G loss:21.607529]\n",
      "274 [D loss: 0.016883, acc.:100.00%] [G loss:20.285179]\n",
      "275 [D loss: 0.161393, acc.:96.88%] [G loss:20.365047]\n",
      "276 [D loss: 0.043691, acc.:100.00%] [G loss:23.389692]\n",
      "277 [D loss: 0.040550, acc.:100.00%] [G loss:20.900986]\n",
      "278 [D loss: 0.037430, acc.:100.00%] [G loss:21.839705]\n",
      "279 [D loss: 0.054100, acc.:96.88%] [G loss:18.012592]\n",
      "280 [D loss: 0.395298, acc.:87.50%] [G loss:24.676159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281 [D loss: 0.031254, acc.:100.00%] [G loss:30.280186]\n",
      "282 [D loss: 0.205404, acc.:90.62%] [G loss:28.080942]\n",
      "283 [D loss: 0.168057, acc.:93.75%] [G loss:26.076992]\n",
      "284 [D loss: 0.037027, acc.:100.00%] [G loss:25.783264]\n",
      "285 [D loss: 0.053829, acc.:96.88%] [G loss:26.027897]\n",
      "286 [D loss: 0.028124, acc.:100.00%] [G loss:25.917963]\n",
      "287 [D loss: 0.172474, acc.:93.75%] [G loss:25.766357]\n",
      "288 [D loss: 0.039629, acc.:100.00%] [G loss:30.464237]\n",
      "289 [D loss: 0.099055, acc.:96.88%] [G loss:34.235023]\n",
      "290 [D loss: 0.058133, acc.:96.88%] [G loss:28.138783]\n",
      "291 [D loss: 0.046382, acc.:100.00%] [G loss:26.955765]\n",
      "292 [D loss: 0.033044, acc.:100.00%] [G loss:21.475203]\n",
      "293 [D loss: 0.119962, acc.:96.88%] [G loss:20.085720]\n",
      "294 [D loss: 0.012586, acc.:100.00%] [G loss:19.257965]\n",
      "295 [D loss: 0.028164, acc.:100.00%] [G loss:21.242985]\n",
      "296 [D loss: 0.018720, acc.:100.00%] [G loss:20.036598]\n",
      "297 [D loss: 0.132663, acc.:96.88%] [G loss:24.886215]\n",
      "298 [D loss: 0.069787, acc.:96.88%] [G loss:23.642035]\n",
      "299 [D loss: 0.061440, acc.:100.00%] [G loss:24.835346]\n",
      "300 [D loss: 0.026531, acc.:100.00%] [G loss:20.142241]\n",
      "301 [D loss: 0.132356, acc.:96.88%] [G loss:16.306423]\n",
      "302 [D loss: 0.030164, acc.:100.00%] [G loss:18.209126]\n",
      "303 [D loss: 0.020767, acc.:100.00%] [G loss:16.965561]\n",
      "304 [D loss: 0.024398, acc.:100.00%] [G loss:18.816677]\n",
      "305 [D loss: 0.075491, acc.:96.88%] [G loss:20.150354]\n",
      "306 [D loss: 0.021805, acc.:100.00%] [G loss:19.879091]\n",
      "307 [D loss: 0.152469, acc.:96.88%] [G loss:22.020573]\n",
      "308 [D loss: 0.154666, acc.:93.75%] [G loss:26.727955]\n",
      "309 [D loss: 0.153362, acc.:96.88%] [G loss:26.419088]\n",
      "310 [D loss: 0.111917, acc.:96.88%] [G loss:17.899475]\n",
      "311 [D loss: 0.115294, acc.:93.75%] [G loss:18.351116]\n",
      "312 [D loss: 0.010694, acc.:100.00%] [G loss:19.279684]\n",
      "313 [D loss: 0.109214, acc.:96.88%] [G loss:21.576775]\n",
      "314 [D loss: 0.010910, acc.:100.00%] [G loss:28.872356]\n",
      "315 [D loss: 0.082206, acc.:96.88%] [G loss:32.357758]\n",
      "316 [D loss: 0.025225, acc.:100.00%] [G loss:39.273567]\n",
      "317 [D loss: 0.187692, acc.:93.75%] [G loss:37.139179]\n",
      "318 [D loss: 0.031865, acc.:100.00%] [G loss:30.008190]\n",
      "319 [D loss: 0.010762, acc.:100.00%] [G loss:28.822948]\n",
      "320 [D loss: 0.010252, acc.:100.00%] [G loss:20.204447]\n",
      "321 [D loss: 0.079730, acc.:96.88%] [G loss:21.236874]\n",
      "322 [D loss: 0.014660, acc.:100.00%] [G loss:27.765179]\n",
      "323 [D loss: 0.011428, acc.:100.00%] [G loss:32.515739]\n",
      "324 [D loss: 0.019842, acc.:100.00%] [G loss:31.274017]\n",
      "325 [D loss: 0.039023, acc.:100.00%] [G loss:32.051239]\n",
      "326 [D loss: 0.066284, acc.:96.88%] [G loss:27.227379]\n",
      "327 [D loss: 0.132467, acc.:96.88%] [G loss:23.061209]\n",
      "328 [D loss: 0.034221, acc.:100.00%] [G loss:19.678274]\n",
      "329 [D loss: 0.177322, acc.:96.88%] [G loss:20.176739]\n",
      "330 [D loss: 0.056037, acc.:96.88%] [G loss:21.263836]\n",
      "331 [D loss: 0.201547, acc.:96.88%] [G loss:22.390644]\n",
      "332 [D loss: 0.020358, acc.:100.00%] [G loss:28.262281]\n",
      "333 [D loss: 0.087305, acc.:96.88%] [G loss:26.048653]\n",
      "334 [D loss: 0.126983, acc.:96.88%] [G loss:25.228556]\n",
      "335 [D loss: 0.025118, acc.:100.00%] [G loss:23.810991]\n",
      "336 [D loss: 0.033103, acc.:100.00%] [G loss:22.497114]\n",
      "337 [D loss: 0.030457, acc.:100.00%] [G loss:22.089729]\n",
      "338 [D loss: 0.113127, acc.:96.88%] [G loss:20.373867]\n",
      "339 [D loss: 0.027905, acc.:100.00%] [G loss:28.166285]\n",
      "340 [D loss: 0.053736, acc.:96.88%] [G loss:25.853750]\n",
      "341 [D loss: 0.037870, acc.:100.00%] [G loss:24.667536]\n",
      "342 [D loss: 0.032863, acc.:100.00%] [G loss:23.508415]\n",
      "343 [D loss: 0.117493, acc.:93.75%] [G loss:20.228142]\n",
      "344 [D loss: 0.100741, acc.:96.88%] [G loss:21.464592]\n",
      "345 [D loss: 0.058082, acc.:96.88%] [G loss:23.962620]\n",
      "346 [D loss: 0.019418, acc.:100.00%] [G loss:24.701878]\n",
      "347 [D loss: 0.025685, acc.:100.00%] [G loss:27.572927]\n",
      "348 [D loss: 0.028182, acc.:100.00%] [G loss:23.578827]\n",
      "349 [D loss: 0.051646, acc.:100.00%] [G loss:22.887869]\n",
      "350 [D loss: 0.014823, acc.:100.00%] [G loss:25.638824]\n",
      "351 [D loss: 0.217656, acc.:96.88%] [G loss:26.731230]\n",
      "352 [D loss: 0.127655, acc.:96.88%] [G loss:30.370771]\n",
      "353 [D loss: 0.049218, acc.:100.00%] [G loss:29.863829]\n",
      "354 [D loss: 0.067715, acc.:100.00%] [G loss:25.380234]\n",
      "355 [D loss: 0.043620, acc.:100.00%] [G loss:24.699493]\n",
      "356 [D loss: 0.015084, acc.:100.00%] [G loss:22.787363]\n",
      "357 [D loss: 0.008931, acc.:100.00%] [G loss:20.941698]\n",
      "358 [D loss: 0.067697, acc.:96.88%] [G loss:19.251566]\n",
      "359 [D loss: 0.009541, acc.:100.00%] [G loss:19.901197]\n",
      "360 [D loss: 0.010154, acc.:100.00%] [G loss:21.093054]\n",
      "361 [D loss: 0.022976, acc.:100.00%] [G loss:26.980762]\n",
      "362 [D loss: 0.167679, acc.:96.88%] [G loss:30.916435]\n",
      "363 [D loss: 0.062560, acc.:96.88%] [G loss:29.028440]\n",
      "364 [D loss: 0.021979, acc.:100.00%] [G loss:26.476152]\n",
      "365 [D loss: 0.014265, acc.:100.00%] [G loss:22.393200]\n",
      "366 [D loss: 0.389446, acc.:96.88%] [G loss:23.118793]\n",
      "367 [D loss: 0.069511, acc.:96.88%] [G loss:26.523628]\n",
      "368 [D loss: 0.019118, acc.:100.00%] [G loss:29.196589]\n",
      "369 [D loss: 0.032385, acc.:100.00%] [G loss:31.287727]\n",
      "370 [D loss: 0.136585, acc.:93.75%] [G loss:25.325172]\n",
      "371 [D loss: 0.016999, acc.:100.00%] [G loss:18.683361]\n",
      "372 [D loss: 0.073936, acc.:96.88%] [G loss:15.844146]\n",
      "373 [D loss: 0.086015, acc.:93.75%] [G loss:20.925108]\n",
      "374 [D loss: 0.010118, acc.:100.00%] [G loss:25.999775]\n",
      "375 [D loss: 0.011247, acc.:100.00%] [G loss:29.666885]\n",
      "376 [D loss: 0.482556, acc.:90.62%] [G loss:33.019009]\n",
      "377 [D loss: 0.117327, acc.:93.75%] [G loss:30.003242]\n",
      "378 [D loss: 0.021200, acc.:100.00%] [G loss:16.909306]\n",
      "379 [D loss: 0.003469, acc.:100.00%] [G loss:17.541401]\n",
      "380 [D loss: 0.029008, acc.:100.00%] [G loss:14.238188]\n",
      "381 [D loss: 0.301830, acc.:90.62%] [G loss:21.588772]\n",
      "382 [D loss: 0.006041, acc.:100.00%] [G loss:33.021534]\n",
      "383 [D loss: 0.039870, acc.:100.00%] [G loss:45.324745]\n",
      "384 [D loss: 0.265978, acc.:93.75%] [G loss:41.733086]\n",
      "385 [D loss: 0.030751, acc.:100.00%] [G loss:35.905972]\n",
      "386 [D loss: 0.011936, acc.:100.00%] [G loss:32.064667]\n",
      "387 [D loss: 0.006009, acc.:100.00%] [G loss:28.513908]\n",
      "388 [D loss: 0.021175, acc.:100.00%] [G loss:25.101603]\n",
      "389 [D loss: 0.001785, acc.:100.00%] [G loss:27.267792]\n",
      "390 [D loss: 0.003904, acc.:100.00%] [G loss:22.492962]\n",
      "391 [D loss: 0.233912, acc.:96.88%] [G loss:24.338228]\n",
      "392 [D loss: 0.011157, acc.:100.00%] [G loss:27.008293]\n",
      "393 [D loss: 0.007684, acc.:100.00%] [G loss:28.400599]\n",
      "394 [D loss: 0.004492, acc.:100.00%] [G loss:28.474157]\n",
      "395 [D loss: 0.035523, acc.:100.00%] [G loss:30.087673]\n",
      "396 [D loss: 0.011047, acc.:100.00%] [G loss:29.436874]\n",
      "397 [D loss: 0.054232, acc.:96.88%] [G loss:28.242973]\n",
      "398 [D loss: 0.074745, acc.:96.88%] [G loss:26.032412]\n",
      "399 [D loss: 0.015006, acc.:100.00%] [G loss:22.492065]\n",
      "400 [D loss: 0.011614, acc.:100.00%] [G loss:19.673161]\n",
      "401 [D loss: 0.061366, acc.:96.88%] [G loss:19.933990]\n",
      "402 [D loss: 0.011644, acc.:100.00%] [G loss:18.727558]\n",
      "403 [D loss: 0.118520, acc.:96.88%] [G loss:16.693024]\n",
      "404 [D loss: 0.008985, acc.:100.00%] [G loss:18.962864]\n",
      "405 [D loss: 0.008064, acc.:100.00%] [G loss:19.355827]\n",
      "406 [D loss: 0.013460, acc.:100.00%] [G loss:23.485737]\n",
      "407 [D loss: 0.021309, acc.:100.00%] [G loss:22.185429]\n",
      "408 [D loss: 0.022306, acc.:100.00%] [G loss:19.476028]\n",
      "409 [D loss: 0.009999, acc.:100.00%] [G loss:16.181129]\n",
      "410 [D loss: 0.018497, acc.:100.00%] [G loss:13.351677]\n",
      "411 [D loss: 0.012303, acc.:100.00%] [G loss:11.442863]\n",
      "412 [D loss: 0.074367, acc.:96.88%] [G loss:10.852409]\n",
      "413 [D loss: 0.005459, acc.:100.00%] [G loss:15.682702]\n",
      "414 [D loss: 0.052803, acc.:96.88%] [G loss:17.405960]\n",
      "415 [D loss: 0.020536, acc.:100.00%] [G loss:22.366161]\n",
      "416 [D loss: 0.070193, acc.:100.00%] [G loss:20.164742]\n",
      "417 [D loss: 0.073493, acc.:96.88%] [G loss:18.104279]\n",
      "418 [D loss: 0.112506, acc.:93.75%] [G loss:17.902702]\n",
      "419 [D loss: 0.036465, acc.:100.00%] [G loss:24.961014]\n",
      "420 [D loss: 0.044064, acc.:100.00%] [G loss:31.471258]\n",
      "421 [D loss: 0.249403, acc.:90.62%] [G loss:31.057053]\n",
      "422 [D loss: 0.063957, acc.:96.88%] [G loss:34.850407]\n",
      "423 [D loss: 0.025354, acc.:100.00%] [G loss:35.915188]\n",
      "424 [D loss: 0.035914, acc.:96.88%] [G loss:38.713596]\n",
      "425 [D loss: 0.031412, acc.:100.00%] [G loss:32.236702]\n",
      "426 [D loss: 0.015102, acc.:100.00%] [G loss:32.582378]\n",
      "427 [D loss: 0.011139, acc.:100.00%] [G loss:29.672619]\n",
      "428 [D loss: 0.015177, acc.:100.00%] [G loss:32.328869]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 [D loss: 0.008436, acc.:100.00%] [G loss:31.081802]\n",
      "430 [D loss: 0.005768, acc.:100.00%] [G loss:29.469057]\n",
      "431 [D loss: 0.006571, acc.:100.00%] [G loss:23.225374]\n",
      "432 [D loss: 0.012885, acc.:100.00%] [G loss:22.569136]\n",
      "433 [D loss: 0.011073, acc.:100.00%] [G loss:24.133787]\n",
      "434 [D loss: 0.041592, acc.:96.88%] [G loss:26.657228]\n",
      "435 [D loss: 0.010650, acc.:100.00%] [G loss:24.277203]\n",
      "436 [D loss: 0.009292, acc.:100.00%] [G loss:24.436632]\n",
      "437 [D loss: 0.010393, acc.:100.00%] [G loss:23.005756]\n",
      "438 [D loss: 0.014595, acc.:100.00%] [G loss:24.726898]\n",
      "439 [D loss: 0.191681, acc.:90.62%] [G loss:19.977100]\n",
      "440 [D loss: 0.069081, acc.:96.88%] [G loss:25.001085]\n",
      "441 [D loss: 0.063752, acc.:96.88%] [G loss:31.255970]\n",
      "442 [D loss: 0.066773, acc.:96.88%] [G loss:34.374916]\n",
      "443 [D loss: 0.066937, acc.:96.88%] [G loss:29.800293]\n",
      "444 [D loss: 0.134677, acc.:93.75%] [G loss:31.049273]\n",
      "445 [D loss: 0.007963, acc.:100.00%] [G loss:32.697762]\n",
      "446 [D loss: 0.009094, acc.:100.00%] [G loss:29.470936]\n",
      "447 [D loss: 0.016240, acc.:100.00%] [G loss:32.905144]\n",
      "448 [D loss: 0.011202, acc.:100.00%] [G loss:33.054111]\n",
      "449 [D loss: 0.009306, acc.:100.00%] [G loss:34.392315]\n",
      "450 [D loss: 0.114248, acc.:96.88%] [G loss:35.157204]\n",
      "451 [D loss: 0.008604, acc.:100.00%] [G loss:39.968658]\n",
      "452 [D loss: 0.159614, acc.:90.62%] [G loss:39.306339]\n",
      "453 [D loss: 0.298863, acc.:93.75%] [G loss:34.054493]\n",
      "454 [D loss: 0.010895, acc.:100.00%] [G loss:27.978823]\n",
      "455 [D loss: 0.192518, acc.:96.88%] [G loss:28.437834]\n",
      "456 [D loss: 0.004308, acc.:100.00%] [G loss:27.764170]\n",
      "457 [D loss: 0.008423, acc.:100.00%] [G loss:30.185371]\n",
      "458 [D loss: 0.009031, acc.:100.00%] [G loss:27.244505]\n",
      "459 [D loss: 0.006230, acc.:100.00%] [G loss:26.173857]\n",
      "460 [D loss: 0.008823, acc.:100.00%] [G loss:27.611389]\n",
      "461 [D loss: 0.121037, acc.:96.88%] [G loss:30.026756]\n",
      "462 [D loss: 0.025016, acc.:100.00%] [G loss:29.471256]\n",
      "463 [D loss: 0.048046, acc.:96.88%] [G loss:32.029762]\n",
      "464 [D loss: 0.090871, acc.:96.88%] [G loss:26.562603]\n",
      "465 [D loss: 0.018491, acc.:100.00%] [G loss:25.135403]\n",
      "466 [D loss: 0.020293, acc.:100.00%] [G loss:22.020735]\n",
      "467 [D loss: 0.231000, acc.:90.62%] [G loss:33.311764]\n",
      "468 [D loss: 0.012958, acc.:100.00%] [G loss:42.836422]\n",
      "469 [D loss: 0.146816, acc.:93.75%] [G loss:42.212612]\n",
      "470 [D loss: 0.039906, acc.:100.00%] [G loss:41.660851]\n",
      "471 [D loss: 0.020790, acc.:100.00%] [G loss:41.508411]\n",
      "472 [D loss: 0.235237, acc.:96.88%] [G loss:42.038383]\n",
      "473 [D loss: 0.034344, acc.:96.88%] [G loss:40.276215]\n",
      "474 [D loss: 0.010435, acc.:100.00%] [G loss:48.160210]\n",
      "475 [D loss: 0.004932, acc.:100.00%] [G loss:40.770958]\n",
      "476 [D loss: 0.014104, acc.:100.00%] [G loss:38.537205]\n",
      "477 [D loss: 0.015784, acc.:100.00%] [G loss:38.934616]\n",
      "478 [D loss: 0.017829, acc.:100.00%] [G loss:37.495148]\n",
      "479 [D loss: 0.008024, acc.:100.00%] [G loss:33.578896]\n",
      "480 [D loss: 0.012331, acc.:100.00%] [G loss:34.474907]\n",
      "481 [D loss: 0.012479, acc.:100.00%] [G loss:28.682583]\n",
      "482 [D loss: 0.003565, acc.:100.00%] [G loss:24.536177]\n",
      "483 [D loss: 0.007755, acc.:100.00%] [G loss:22.743683]\n",
      "484 [D loss: 0.003573, acc.:100.00%] [G loss:21.773628]\n",
      "485 [D loss: 0.003931, acc.:100.00%] [G loss:16.641230]\n",
      "486 [D loss: 0.156148, acc.:96.88%] [G loss:18.235168]\n",
      "487 [D loss: 0.006471, acc.:100.00%] [G loss:26.037064]\n",
      "488 [D loss: 0.080023, acc.:96.88%] [G loss:27.150591]\n",
      "489 [D loss: 0.100938, acc.:93.75%] [G loss:23.251799]\n",
      "490 [D loss: 0.009317, acc.:100.00%] [G loss:19.285522]\n",
      "491 [D loss: 0.094968, acc.:96.88%] [G loss:16.317543]\n",
      "492 [D loss: 0.027585, acc.:100.00%] [G loss:19.709297]\n",
      "493 [D loss: 0.019952, acc.:100.00%] [G loss:21.887905]\n",
      "494 [D loss: 0.030365, acc.:100.00%] [G loss:23.991219]\n",
      "495 [D loss: 0.016366, acc.:100.00%] [G loss:20.340633]\n",
      "496 [D loss: 0.038799, acc.:96.88%] [G loss:21.551041]\n",
      "497 [D loss: 0.030284, acc.:100.00%] [G loss:24.373198]\n",
      "498 [D loss: 0.029910, acc.:100.00%] [G loss:24.133301]\n",
      "499 [D loss: 0.058017, acc.:96.88%] [G loss:27.805515]\n",
      "500 [D loss: 0.012787, acc.:100.00%] [G loss:24.715548]\n",
      "501 [D loss: 0.035237, acc.:100.00%] [G loss:30.236080]\n",
      "502 [D loss: 0.034418, acc.:100.00%] [G loss:27.961676]\n",
      "503 [D loss: 0.065423, acc.:100.00%] [G loss:21.168114]\n",
      "504 [D loss: 0.042972, acc.:96.88%] [G loss:20.606440]\n",
      "505 [D loss: 0.005509, acc.:100.00%] [G loss:21.633347]\n",
      "506 [D loss: 0.031987, acc.:96.88%] [G loss:23.066603]\n",
      "507 [D loss: 0.061202, acc.:96.88%] [G loss:28.668364]\n",
      "508 [D loss: 0.006861, acc.:100.00%] [G loss:31.707680]\n",
      "509 [D loss: 0.042259, acc.:96.88%] [G loss:34.570091]\n",
      "510 [D loss: 0.042344, acc.:100.00%] [G loss:33.398766]\n",
      "511 [D loss: 0.023116, acc.:100.00%] [G loss:30.938915]\n",
      "512 [D loss: 0.017041, acc.:100.00%] [G loss:25.601276]\n",
      "513 [D loss: 0.161482, acc.:96.88%] [G loss:28.332603]\n",
      "514 [D loss: 0.016897, acc.:100.00%] [G loss:37.250275]\n",
      "515 [D loss: 0.077755, acc.:93.75%] [G loss:36.202637]\n",
      "516 [D loss: 0.312462, acc.:93.75%] [G loss:33.838066]\n",
      "517 [D loss: 0.058796, acc.:96.88%] [G loss:34.456596]\n",
      "518 [D loss: 0.015011, acc.:100.00%] [G loss:28.531443]\n",
      "519 [D loss: 0.008576, acc.:100.00%] [G loss:32.279701]\n",
      "520 [D loss: 0.017660, acc.:100.00%] [G loss:29.324184]\n",
      "521 [D loss: 0.165641, acc.:96.88%] [G loss:31.111404]\n",
      "522 [D loss: 0.003560, acc.:100.00%] [G loss:38.561699]\n",
      "523 [D loss: 0.051325, acc.:96.88%] [G loss:38.389236]\n",
      "524 [D loss: 0.036383, acc.:100.00%] [G loss:40.186005]\n",
      "525 [D loss: 0.014383, acc.:100.00%] [G loss:38.595226]\n",
      "526 [D loss: 0.024143, acc.:100.00%] [G loss:36.181206]\n",
      "527 [D loss: 0.013001, acc.:100.00%] [G loss:29.373777]\n",
      "528 [D loss: 0.019294, acc.:100.00%] [G loss:29.882507]\n",
      "529 [D loss: 0.101005, acc.:96.88%] [G loss:24.267809]\n",
      "530 [D loss: 0.002342, acc.:100.00%] [G loss:28.755379]\n",
      "531 [D loss: 0.007900, acc.:100.00%] [G loss:31.838770]\n",
      "532 [D loss: 0.059875, acc.:96.88%] [G loss:32.553947]\n",
      "533 [D loss: 0.006713, acc.:100.00%] [G loss:31.661104]\n",
      "534 [D loss: 0.011429, acc.:100.00%] [G loss:29.438332]\n",
      "535 [D loss: 0.033936, acc.:100.00%] [G loss:28.389282]\n",
      "536 [D loss: 0.008069, acc.:100.00%] [G loss:32.916061]\n",
      "537 [D loss: 0.028419, acc.:100.00%] [G loss:32.122559]\n",
      "538 [D loss: 0.019330, acc.:100.00%] [G loss:31.214165]\n",
      "539 [D loss: 0.054964, acc.:96.88%] [G loss:29.912674]\n",
      "540 [D loss: 0.019264, acc.:100.00%] [G loss:29.043056]\n",
      "541 [D loss: 0.128198, acc.:93.75%] [G loss:30.340729]\n",
      "542 [D loss: 0.013245, acc.:100.00%] [G loss:35.965092]\n",
      "543 [D loss: 0.016177, acc.:100.00%] [G loss:47.645741]\n",
      "544 [D loss: 0.120775, acc.:93.75%] [G loss:41.108398]\n",
      "545 [D loss: 0.021109, acc.:100.00%] [G loss:43.391808]\n",
      "546 [D loss: 0.047130, acc.:100.00%] [G loss:44.557144]\n",
      "547 [D loss: 0.009958, acc.:100.00%] [G loss:44.151173]\n",
      "548 [D loss: 0.002211, acc.:100.00%] [G loss:39.429367]\n",
      "549 [D loss: 0.002729, acc.:100.00%] [G loss:36.121986]\n",
      "550 [D loss: 0.002844, acc.:100.00%] [G loss:42.789822]\n",
      "551 [D loss: 0.003906, acc.:100.00%] [G loss:43.262814]\n",
      "552 [D loss: 0.002880, acc.:100.00%] [G loss:40.471672]\n",
      "553 [D loss: 0.003241, acc.:100.00%] [G loss:39.198036]\n",
      "554 [D loss: 0.001766, acc.:100.00%] [G loss:41.857735]\n",
      "555 [D loss: 0.000996, acc.:100.00%] [G loss:34.339134]\n",
      "556 [D loss: 0.001455, acc.:100.00%] [G loss:36.723763]\n",
      "557 [D loss: 0.002640, acc.:100.00%] [G loss:29.389111]\n",
      "558 [D loss: 0.019833, acc.:100.00%] [G loss:34.900986]\n",
      "559 [D loss: 0.030182, acc.:96.88%] [G loss:32.099419]\n",
      "560 [D loss: 0.003173, acc.:100.00%] [G loss:35.824322]\n",
      "561 [D loss: 0.008139, acc.:100.00%] [G loss:40.071613]\n",
      "562 [D loss: 0.028031, acc.:100.00%] [G loss:40.582623]\n",
      "563 [D loss: 0.067920, acc.:96.88%] [G loss:32.945366]\n",
      "564 [D loss: 0.190653, acc.:90.62%] [G loss:29.120560]\n",
      "565 [D loss: 0.007576, acc.:100.00%] [G loss:35.800114]\n",
      "566 [D loss: 0.030273, acc.:100.00%] [G loss:39.806778]\n",
      "567 [D loss: 0.006433, acc.:100.00%] [G loss:49.454094]\n",
      "568 [D loss: 0.022857, acc.:100.00%] [G loss:44.219913]\n",
      "569 [D loss: 0.070815, acc.:96.88%] [G loss:45.047607]\n",
      "570 [D loss: 0.007576, acc.:100.00%] [G loss:43.092396]\n",
      "571 [D loss: 0.002306, acc.:100.00%] [G loss:37.400177]\n",
      "572 [D loss: 0.306016, acc.:96.88%] [G loss:32.299252]\n",
      "573 [D loss: 0.013526, acc.:100.00%] [G loss:37.271076]\n",
      "574 [D loss: 0.006632, acc.:100.00%] [G loss:44.892029]\n",
      "575 [D loss: 0.012008, acc.:100.00%] [G loss:43.146656]\n",
      "576 [D loss: 0.037528, acc.:100.00%] [G loss:48.805592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577 [D loss: 0.047424, acc.:100.00%] [G loss:44.542892]\n",
      "578 [D loss: 0.150201, acc.:93.75%] [G loss:33.907349]\n",
      "579 [D loss: 0.005238, acc.:100.00%] [G loss:30.854643]\n",
      "580 [D loss: 0.005931, acc.:100.00%] [G loss:28.174984]\n",
      "581 [D loss: 0.084802, acc.:93.75%] [G loss:23.330893]\n",
      "582 [D loss: 0.002381, acc.:100.00%] [G loss:29.644543]\n",
      "583 [D loss: 0.047973, acc.:96.88%] [G loss:34.824352]\n",
      "584 [D loss: 0.009855, acc.:100.00%] [G loss:44.967491]\n",
      "585 [D loss: 0.062949, acc.:96.88%] [G loss:41.918243]\n",
      "586 [D loss: 0.054194, acc.:100.00%] [G loss:40.060101]\n",
      "587 [D loss: 0.025499, acc.:100.00%] [G loss:40.245472]\n",
      "588 [D loss: 0.006000, acc.:100.00%] [G loss:31.951349]\n",
      "589 [D loss: 0.005434, acc.:100.00%] [G loss:27.515993]\n",
      "590 [D loss: 0.002439, acc.:100.00%] [G loss:25.945278]\n",
      "591 [D loss: 0.001489, acc.:100.00%] [G loss:22.729153]\n",
      "592 [D loss: 0.002695, acc.:100.00%] [G loss:18.370872]\n",
      "593 [D loss: 0.047662, acc.:96.88%] [G loss:22.847160]\n",
      "594 [D loss: 0.002934, acc.:100.00%] [G loss:27.436607]\n",
      "595 [D loss: 0.004424, acc.:100.00%] [G loss:33.502419]\n",
      "596 [D loss: 0.014356, acc.:100.00%] [G loss:28.173367]\n",
      "597 [D loss: 0.020527, acc.:100.00%] [G loss:31.056351]\n",
      "598 [D loss: 0.008566, acc.:100.00%] [G loss:29.229654]\n",
      "599 [D loss: 0.010072, acc.:100.00%] [G loss:30.135429]\n",
      "600 [D loss: 0.041824, acc.:96.88%] [G loss:28.955683]\n",
      "601 [D loss: 0.016689, acc.:100.00%] [G loss:29.763582]\n",
      "602 [D loss: 0.044254, acc.:96.88%] [G loss:23.397949]\n",
      "603 [D loss: 0.007383, acc.:100.00%] [G loss:24.946106]\n",
      "604 [D loss: 0.022160, acc.:100.00%] [G loss:26.826452]\n",
      "605 [D loss: 0.002969, acc.:100.00%] [G loss:27.129242]\n",
      "606 [D loss: 0.003285, acc.:100.00%] [G loss:29.138332]\n",
      "607 [D loss: 0.004148, acc.:100.00%] [G loss:31.175018]\n",
      "608 [D loss: 0.004189, acc.:100.00%] [G loss:31.550989]\n",
      "609 [D loss: 0.174428, acc.:96.88%] [G loss:35.647354]\n",
      "610 [D loss: 0.008251, acc.:100.00%] [G loss:37.573662]\n",
      "611 [D loss: 0.015375, acc.:100.00%] [G loss:41.117924]\n",
      "612 [D loss: 0.087788, acc.:93.75%] [G loss:35.849590]\n",
      "613 [D loss: 0.003376, acc.:100.00%] [G loss:27.341816]\n",
      "614 [D loss: 0.003826, acc.:100.00%] [G loss:31.973816]\n",
      "615 [D loss: 0.127348, acc.:96.88%] [G loss:29.826813]\n",
      "616 [D loss: 0.002144, acc.:100.00%] [G loss:40.386520]\n",
      "617 [D loss: 0.007301, acc.:100.00%] [G loss:42.742485]\n",
      "618 [D loss: 0.024804, acc.:100.00%] [G loss:47.949463]\n",
      "619 [D loss: 0.039304, acc.:100.00%] [G loss:44.686039]\n",
      "620 [D loss: 0.126724, acc.:93.75%] [G loss:33.328629]\n",
      "621 [D loss: 0.363675, acc.:96.88%] [G loss:26.775358]\n",
      "622 [D loss: 0.001085, acc.:100.00%] [G loss:28.147726]\n",
      "623 [D loss: 0.000988, acc.:100.00%] [G loss:24.200573]\n",
      "624 [D loss: 0.110400, acc.:96.88%] [G loss:26.801968]\n",
      "625 [D loss: 0.001196, acc.:100.00%] [G loss:35.024727]\n",
      "626 [D loss: 0.005076, acc.:100.00%] [G loss:34.217102]\n",
      "627 [D loss: 0.001671, acc.:100.00%] [G loss:41.131207]\n",
      "628 [D loss: 0.004074, acc.:100.00%] [G loss:42.044685]\n",
      "629 [D loss: 0.004475, acc.:100.00%] [G loss:46.066452]\n",
      "630 [D loss: 0.005365, acc.:100.00%] [G loss:42.612389]\n",
      "631 [D loss: 0.011614, acc.:100.00%] [G loss:41.173840]\n",
      "632 [D loss: 0.008122, acc.:100.00%] [G loss:41.368225]\n",
      "633 [D loss: 0.009689, acc.:100.00%] [G loss:39.201405]\n",
      "634 [D loss: 0.010044, acc.:100.00%] [G loss:37.325844]\n",
      "635 [D loss: 0.006283, acc.:100.00%] [G loss:34.449192]\n",
      "636 [D loss: 0.013379, acc.:100.00%] [G loss:26.599007]\n",
      "637 [D loss: 0.003350, acc.:100.00%] [G loss:30.286358]\n",
      "638 [D loss: 0.014483, acc.:100.00%] [G loss:26.766541]\n",
      "639 [D loss: 0.015777, acc.:100.00%] [G loss:26.868015]\n",
      "640 [D loss: 0.005322, acc.:100.00%] [G loss:31.723263]\n",
      "641 [D loss: 0.006308, acc.:100.00%] [G loss:30.567118]\n",
      "642 [D loss: 0.002544, acc.:100.00%] [G loss:29.543180]\n",
      "643 [D loss: 0.010243, acc.:100.00%] [G loss:30.837919]\n",
      "644 [D loss: 0.075174, acc.:93.75%] [G loss:28.959974]\n",
      "645 [D loss: 0.170762, acc.:93.75%] [G loss:29.859631]\n",
      "646 [D loss: 0.004265, acc.:100.00%] [G loss:43.096176]\n",
      "647 [D loss: 0.013302, acc.:100.00%] [G loss:48.647942]\n",
      "648 [D loss: 0.068391, acc.:96.88%] [G loss:54.811749]\n",
      "649 [D loss: 0.023016, acc.:100.00%] [G loss:65.373688]\n",
      "650 [D loss: 0.025538, acc.:100.00%] [G loss:56.910999]\n",
      "651 [D loss: 0.276959, acc.:93.75%] [G loss:60.878033]\n",
      "652 [D loss: 0.006100, acc.:100.00%] [G loss:67.051331]\n",
      "653 [D loss: 0.060451, acc.:100.00%] [G loss:84.257690]\n",
      "654 [D loss: 0.116389, acc.:90.62%] [G loss:60.582466]\n",
      "655 [D loss: 0.030314, acc.:100.00%] [G loss:77.295593]\n",
      "656 [D loss: 0.002657, acc.:100.00%] [G loss:66.628311]\n",
      "657 [D loss: 0.001992, acc.:100.00%] [G loss:63.273544]\n",
      "658 [D loss: 0.001060, acc.:100.00%] [G loss:62.393703]\n",
      "659 [D loss: 0.001001, acc.:100.00%] [G loss:49.595985]\n",
      "660 [D loss: 0.046500, acc.:96.88%] [G loss:64.027374]\n",
      "661 [D loss: 0.062939, acc.:93.75%] [G loss:57.084702]\n",
      "662 [D loss: 0.001656, acc.:100.00%] [G loss:64.136185]\n",
      "663 [D loss: 0.003711, acc.:100.00%] [G loss:70.904289]\n",
      "664 [D loss: 0.008924, acc.:100.00%] [G loss:64.963242]\n",
      "665 [D loss: 0.033691, acc.:100.00%] [G loss:64.601089]\n",
      "666 [D loss: 0.073994, acc.:96.88%] [G loss:58.267735]\n",
      "667 [D loss: 0.007097, acc.:100.00%] [G loss:51.013508]\n",
      "668 [D loss: 0.001423, acc.:100.00%] [G loss:45.975983]\n",
      "669 [D loss: 0.002685, acc.:100.00%] [G loss:41.365761]\n",
      "670 [D loss: 0.001479, acc.:100.00%] [G loss:33.991833]\n",
      "671 [D loss: 0.001519, acc.:100.00%] [G loss:30.108810]\n",
      "672 [D loss: 0.001897, acc.:100.00%] [G loss:25.744307]\n",
      "673 [D loss: 0.042975, acc.:96.88%] [G loss:24.477247]\n",
      "674 [D loss: 0.000766, acc.:100.00%] [G loss:25.966877]\n",
      "675 [D loss: 0.001382, acc.:100.00%] [G loss:28.505127]\n",
      "676 [D loss: 0.002630, acc.:100.00%] [G loss:29.762480]\n",
      "677 [D loss: 0.001174, acc.:100.00%] [G loss:30.773222]\n",
      "678 [D loss: 0.007247, acc.:100.00%] [G loss:31.268257]\n",
      "679 [D loss: 0.002870, acc.:100.00%] [G loss:26.971607]\n",
      "680 [D loss: 0.009348, acc.:100.00%] [G loss:28.067844]\n",
      "681 [D loss: 0.049523, acc.:96.88%] [G loss:28.260542]\n",
      "682 [D loss: 0.016921, acc.:100.00%] [G loss:36.972839]\n",
      "683 [D loss: 0.011790, acc.:100.00%] [G loss:31.771435]\n",
      "684 [D loss: 0.006422, acc.:100.00%] [G loss:29.111473]\n",
      "685 [D loss: 0.005303, acc.:100.00%] [G loss:26.894550]\n",
      "686 [D loss: 0.112969, acc.:96.88%] [G loss:31.904770]\n",
      "687 [D loss: 0.012681, acc.:100.00%] [G loss:31.735939]\n",
      "688 [D loss: 0.014509, acc.:100.00%] [G loss:33.686050]\n",
      "689 [D loss: 0.040524, acc.:96.88%] [G loss:33.688072]\n",
      "690 [D loss: 0.006111, acc.:100.00%] [G loss:30.614307]\n",
      "691 [D loss: 0.022458, acc.:100.00%] [G loss:31.303352]\n",
      "692 [D loss: 0.004030, acc.:100.00%] [G loss:26.963850]\n",
      "693 [D loss: 0.003342, acc.:100.00%] [G loss:26.071732]\n",
      "694 [D loss: 0.067426, acc.:96.88%] [G loss:25.475864]\n",
      "695 [D loss: 0.004378, acc.:100.00%] [G loss:32.076027]\n",
      "696 [D loss: 0.006921, acc.:100.00%] [G loss:30.132671]\n",
      "697 [D loss: 0.004226, acc.:100.00%] [G loss:32.556515]\n",
      "698 [D loss: 0.004276, acc.:100.00%] [G loss:33.581253]\n",
      "699 [D loss: 0.026436, acc.:100.00%] [G loss:35.197701]\n",
      "700 [D loss: 0.014244, acc.:100.00%] [G loss:30.358673]\n",
      "701 [D loss: 0.020010, acc.:100.00%] [G loss:30.484594]\n",
      "702 [D loss: 0.004414, acc.:100.00%] [G loss:32.941765]\n",
      "703 [D loss: 0.016649, acc.:100.00%] [G loss:29.415115]\n",
      "704 [D loss: 0.005898, acc.:100.00%] [G loss:29.597401]\n",
      "705 [D loss: 0.006302, acc.:100.00%] [G loss:25.259979]\n",
      "706 [D loss: 0.004404, acc.:100.00%] [G loss:29.190266]\n",
      "707 [D loss: 0.037195, acc.:96.88%] [G loss:25.154486]\n",
      "708 [D loss: 0.003708, acc.:100.00%] [G loss:26.571243]\n",
      "709 [D loss: 0.003040, acc.:100.00%] [G loss:26.097315]\n",
      "710 [D loss: 0.005115, acc.:100.00%] [G loss:26.034866]\n",
      "711 [D loss: 0.005106, acc.:100.00%] [G loss:28.659098]\n",
      "712 [D loss: 0.004304, acc.:100.00%] [G loss:25.986004]\n",
      "713 [D loss: 0.007818, acc.:100.00%] [G loss:26.767792]\n",
      "714 [D loss: 0.004529, acc.:100.00%] [G loss:23.888657]\n",
      "715 [D loss: 0.004477, acc.:100.00%] [G loss:25.437931]\n",
      "716 [D loss: 0.008158, acc.:100.00%] [G loss:18.105030]\n",
      "717 [D loss: 0.077435, acc.:96.88%] [G loss:24.813751]\n",
      "718 [D loss: 0.006550, acc.:100.00%] [G loss:28.660629]\n",
      "719 [D loss: 0.010433, acc.:100.00%] [G loss:31.421240]\n",
      "720 [D loss: 0.233335, acc.:93.75%] [G loss:30.725079]\n",
      "721 [D loss: 0.011349, acc.:100.00%] [G loss:34.435654]\n",
      "722 [D loss: 0.010519, acc.:100.00%] [G loss:38.177345]\n",
      "723 [D loss: 0.005562, acc.:100.00%] [G loss:36.584824]\n",
      "724 [D loss: 0.004545, acc.:100.00%] [G loss:45.092316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725 [D loss: 0.001677, acc.:100.00%] [G loss:40.816788]\n",
      "726 [D loss: 0.002770, acc.:100.00%] [G loss:45.471779]\n",
      "727 [D loss: 0.056313, acc.:96.88%] [G loss:49.748123]\n",
      "728 [D loss: 0.022254, acc.:100.00%] [G loss:47.025314]\n",
      "729 [D loss: 0.012913, acc.:100.00%] [G loss:47.669914]\n",
      "730 [D loss: 0.017491, acc.:100.00%] [G loss:46.197639]\n",
      "731 [D loss: 0.004955, acc.:100.00%] [G loss:46.743187]\n",
      "732 [D loss: 0.004093, acc.:100.00%] [G loss:48.048183]\n",
      "733 [D loss: 0.006486, acc.:100.00%] [G loss:44.191292]\n",
      "734 [D loss: 0.004343, acc.:100.00%] [G loss:40.070103]\n",
      "735 [D loss: 0.002052, acc.:100.00%] [G loss:37.029579]\n",
      "736 [D loss: 0.004790, acc.:100.00%] [G loss:34.994434]\n",
      "737 [D loss: 0.002493, acc.:100.00%] [G loss:27.951366]\n",
      "738 [D loss: 0.028516, acc.:96.88%] [G loss:34.897148]\n",
      "739 [D loss: 0.002350, acc.:100.00%] [G loss:37.089630]\n",
      "740 [D loss: 0.002533, acc.:100.00%] [G loss:32.974701]\n",
      "741 [D loss: 0.003981, acc.:100.00%] [G loss:37.026169]\n",
      "742 [D loss: 0.007888, acc.:100.00%] [G loss:38.406387]\n",
      "743 [D loss: 0.008703, acc.:100.00%] [G loss:38.870529]\n",
      "744 [D loss: 0.016842, acc.:100.00%] [G loss:36.286541]\n",
      "745 [D loss: 0.014155, acc.:100.00%] [G loss:33.498562]\n",
      "746 [D loss: 0.009227, acc.:100.00%] [G loss:28.355804]\n",
      "747 [D loss: 0.024476, acc.:100.00%] [G loss:29.765841]\n",
      "748 [D loss: 0.002798, acc.:100.00%] [G loss:28.213190]\n",
      "749 [D loss: 0.013671, acc.:100.00%] [G loss:30.286957]\n",
      "750 [D loss: 0.006580, acc.:100.00%] [G loss:28.734398]\n",
      "751 [D loss: 0.004339, acc.:100.00%] [G loss:27.654907]\n",
      "752 [D loss: 0.015600, acc.:100.00%] [G loss:23.717449]\n",
      "753 [D loss: 0.003506, acc.:100.00%] [G loss:23.619438]\n",
      "754 [D loss: 0.006302, acc.:100.00%] [G loss:19.850277]\n",
      "755 [D loss: 0.133231, acc.:93.75%] [G loss:26.535576]\n",
      "756 [D loss: 0.002958, acc.:100.00%] [G loss:40.786873]\n",
      "757 [D loss: 0.017711, acc.:100.00%] [G loss:51.668114]\n",
      "758 [D loss: 0.132458, acc.:93.75%] [G loss:51.066830]\n",
      "759 [D loss: 0.634304, acc.:84.38%] [G loss:63.632030]\n",
      "760 [D loss: 0.129162, acc.:93.75%] [G loss:92.221085]\n",
      "761 [D loss: 1.487256, acc.:65.62%] [G loss:71.664825]\n",
      "762 [D loss: 0.003685, acc.:100.00%] [G loss:59.459885]\n",
      "763 [D loss: 0.130979, acc.:96.88%] [G loss:78.764908]\n",
      "764 [D loss: 0.160420, acc.:96.88%] [G loss:73.220749]\n",
      "765 [D loss: 0.000851, acc.:100.00%] [G loss:82.853867]\n",
      "766 [D loss: 0.000587, acc.:100.00%] [G loss:86.147415]\n",
      "767 [D loss: 0.013144, acc.:100.00%] [G loss:87.382935]\n",
      "768 [D loss: 0.364246, acc.:93.75%] [G loss:85.460564]\n",
      "769 [D loss: 0.009620, acc.:100.00%] [G loss:88.722427]\n",
      "770 [D loss: 0.007894, acc.:100.00%] [G loss:91.809631]\n",
      "771 [D loss: 0.009390, acc.:100.00%] [G loss:85.592300]\n",
      "772 [D loss: 0.072598, acc.:96.88%] [G loss:76.510345]\n",
      "773 [D loss: 0.063217, acc.:96.88%] [G loss:59.767803]\n",
      "774 [D loss: 0.009143, acc.:100.00%] [G loss:60.718079]\n",
      "775 [D loss: 0.003469, acc.:100.00%] [G loss:50.107468]\n",
      "776 [D loss: 0.000580, acc.:100.00%] [G loss:43.985634]\n",
      "777 [D loss: 0.001466, acc.:100.00%] [G loss:33.891777]\n",
      "778 [D loss: 0.001276, acc.:100.00%] [G loss:31.142159]\n",
      "779 [D loss: 0.004248, acc.:100.00%] [G loss:24.114870]\n",
      "780 [D loss: 0.001312, acc.:100.00%] [G loss:22.488298]\n",
      "781 [D loss: 0.002751, acc.:100.00%] [G loss:21.157724]\n",
      "782 [D loss: 0.362405, acc.:93.75%] [G loss:21.580954]\n",
      "783 [D loss: 0.010044, acc.:100.00%] [G loss:19.097435]\n",
      "784 [D loss: 0.001663, acc.:100.00%] [G loss:32.334415]\n",
      "785 [D loss: 0.007327, acc.:100.00%] [G loss:34.494400]\n",
      "786 [D loss: 0.017328, acc.:100.00%] [G loss:43.021317]\n",
      "787 [D loss: 0.069265, acc.:96.88%] [G loss:45.132339]\n",
      "788 [D loss: 0.205318, acc.:90.62%] [G loss:42.711807]\n",
      "789 [D loss: 0.011824, acc.:100.00%] [G loss:42.908306]\n",
      "790 [D loss: 0.008131, acc.:100.00%] [G loss:39.735428]\n",
      "791 [D loss: 0.007964, acc.:100.00%] [G loss:39.471096]\n",
      "792 [D loss: 0.025176, acc.:100.00%] [G loss:36.453094]\n",
      "793 [D loss: 0.015866, acc.:100.00%] [G loss:36.741173]\n",
      "794 [D loss: 0.015646, acc.:100.00%] [G loss:36.324142]\n",
      "795 [D loss: 0.011948, acc.:100.00%] [G loss:32.164783]\n",
      "796 [D loss: 0.045644, acc.:96.88%] [G loss:30.116615]\n",
      "797 [D loss: 0.006557, acc.:100.00%] [G loss:27.976036]\n",
      "798 [D loss: 0.053552, acc.:96.88%] [G loss:23.574501]\n",
      "799 [D loss: 0.004204, acc.:100.00%] [G loss:19.996395]\n",
      "800 [D loss: 0.001887, acc.:100.00%] [G loss:15.920982]\n",
      "801 [D loss: 0.005104, acc.:100.00%] [G loss:17.040791]\n",
      "802 [D loss: 0.032667, acc.:96.88%] [G loss:13.948479]\n",
      "803 [D loss: 0.141884, acc.:96.88%] [G loss:17.751534]\n",
      "804 [D loss: 0.004056, acc.:100.00%] [G loss:19.862610]\n",
      "805 [D loss: 0.006274, acc.:100.00%] [G loss:25.626417]\n",
      "806 [D loss: 0.022486, acc.:100.00%] [G loss:30.680761]\n",
      "807 [D loss: 0.216468, acc.:90.62%] [G loss:19.662262]\n",
      "808 [D loss: 0.018929, acc.:100.00%] [G loss:18.695410]\n",
      "809 [D loss: 0.045771, acc.:96.88%] [G loss:14.087882]\n",
      "810 [D loss: 0.007781, acc.:100.00%] [G loss:15.913954]\n",
      "811 [D loss: 0.047188, acc.:96.88%] [G loss:17.369459]\n",
      "812 [D loss: 0.005436, acc.:100.00%] [G loss:24.596962]\n",
      "813 [D loss: 0.007431, acc.:100.00%] [G loss:28.044003]\n",
      "814 [D loss: 0.013260, acc.:100.00%] [G loss:29.308943]\n",
      "815 [D loss: 0.026665, acc.:100.00%] [G loss:32.465858]\n",
      "816 [D loss: 0.012829, acc.:100.00%] [G loss:30.761751]\n",
      "817 [D loss: 0.012637, acc.:100.00%] [G loss:27.179726]\n",
      "818 [D loss: 0.028439, acc.:100.00%] [G loss:28.900314]\n",
      "819 [D loss: 0.027422, acc.:100.00%] [G loss:27.600327]\n",
      "820 [D loss: 0.005601, acc.:100.00%] [G loss:27.423532]\n",
      "821 [D loss: 0.007062, acc.:100.00%] [G loss:28.769451]\n",
      "822 [D loss: 0.002979, acc.:100.00%] [G loss:27.552713]\n",
      "823 [D loss: 0.006777, acc.:100.00%] [G loss:24.926538]\n",
      "824 [D loss: 0.006437, acc.:100.00%] [G loss:22.081821]\n",
      "825 [D loss: 0.005103, acc.:100.00%] [G loss:20.757900]\n",
      "826 [D loss: 0.003527, acc.:100.00%] [G loss:22.459740]\n",
      "827 [D loss: 0.069767, acc.:96.88%] [G loss:18.300957]\n",
      "828 [D loss: 0.003044, acc.:100.00%] [G loss:19.979610]\n",
      "829 [D loss: 0.001964, acc.:100.00%] [G loss:16.775360]\n",
      "830 [D loss: 0.034061, acc.:96.88%] [G loss:20.887669]\n",
      "831 [D loss: 0.003998, acc.:100.00%] [G loss:23.783617]\n",
      "832 [D loss: 0.027412, acc.:100.00%] [G loss:28.949419]\n",
      "833 [D loss: 0.019644, acc.:100.00%] [G loss:31.458797]\n",
      "834 [D loss: 0.015387, acc.:100.00%] [G loss:34.087280]\n",
      "835 [D loss: 0.014027, acc.:100.00%] [G loss:31.695885]\n",
      "836 [D loss: 0.022821, acc.:100.00%] [G loss:33.491234]\n",
      "837 [D loss: 0.044647, acc.:100.00%] [G loss:24.758652]\n",
      "838 [D loss: 0.030966, acc.:100.00%] [G loss:30.817032]\n",
      "839 [D loss: 0.004641, acc.:100.00%] [G loss:32.233749]\n",
      "840 [D loss: 0.003116, acc.:100.00%] [G loss:29.004539]\n",
      "841 [D loss: 0.004759, acc.:100.00%] [G loss:25.307865]\n",
      "842 [D loss: 0.005961, acc.:100.00%] [G loss:26.966227]\n",
      "843 [D loss: 0.004802, acc.:100.00%] [G loss:24.664646]\n",
      "844 [D loss: 0.011267, acc.:100.00%] [G loss:25.762741]\n",
      "845 [D loss: 0.005384, acc.:100.00%] [G loss:25.374077]\n",
      "846 [D loss: 0.007103, acc.:100.00%] [G loss:22.965916]\n",
      "847 [D loss: 0.008596, acc.:100.00%] [G loss:21.791016]\n",
      "848 [D loss: 0.008255, acc.:100.00%] [G loss:19.654488]\n",
      "849 [D loss: 0.002839, acc.:100.00%] [G loss:18.980816]\n",
      "850 [D loss: 0.002649, acc.:100.00%] [G loss:19.406979]\n",
      "851 [D loss: 0.011178, acc.:100.00%] [G loss:15.681896]\n",
      "852 [D loss: 0.003431, acc.:100.00%] [G loss:17.345482]\n",
      "853 [D loss: 0.003681, acc.:100.00%] [G loss:18.671146]\n",
      "854 [D loss: 0.007144, acc.:100.00%] [G loss:17.185957]\n",
      "855 [D loss: 0.005496, acc.:100.00%] [G loss:19.998924]\n",
      "856 [D loss: 0.060249, acc.:96.88%] [G loss:22.230091]\n",
      "857 [D loss: 0.007160, acc.:100.00%] [G loss:29.708269]\n",
      "858 [D loss: 0.013210, acc.:100.00%] [G loss:33.036354]\n",
      "859 [D loss: 0.020923, acc.:100.00%] [G loss:31.242758]\n",
      "860 [D loss: 0.015293, acc.:100.00%] [G loss:31.978535]\n",
      "861 [D loss: 0.023048, acc.:100.00%] [G loss:33.558449]\n",
      "862 [D loss: 0.079964, acc.:96.88%] [G loss:34.248840]\n",
      "863 [D loss: 0.027338, acc.:100.00%] [G loss:40.709412]\n",
      "864 [D loss: 0.097022, acc.:96.88%] [G loss:35.676620]\n",
      "865 [D loss: 0.008188, acc.:100.00%] [G loss:33.532143]\n",
      "866 [D loss: 0.002551, acc.:100.00%] [G loss:30.496021]\n",
      "867 [D loss: 0.010834, acc.:100.00%] [G loss:29.650389]\n",
      "868 [D loss: 0.002374, acc.:100.00%] [G loss:20.231100]\n",
      "869 [D loss: 0.066774, acc.:96.88%] [G loss:27.794769]\n",
      "870 [D loss: 0.002092, acc.:100.00%] [G loss:31.285522]\n",
      "871 [D loss: 0.008475, acc.:100.00%] [G loss:36.285252]\n",
      "872 [D loss: 0.010934, acc.:100.00%] [G loss:37.101582]\n",
      "873 [D loss: 0.019013, acc.:100.00%] [G loss:38.452183]\n",
      "874 [D loss: 0.011329, acc.:100.00%] [G loss:37.684490]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875 [D loss: 0.015406, acc.:100.00%] [G loss:37.065601]\n",
      "876 [D loss: 0.010111, acc.:100.00%] [G loss:38.295582]\n",
      "877 [D loss: 0.118205, acc.:96.88%] [G loss:42.822800]\n",
      "878 [D loss: 0.009485, acc.:100.00%] [G loss:47.810284]\n",
      "879 [D loss: 0.033239, acc.:100.00%] [G loss:47.751495]\n",
      "880 [D loss: 0.038728, acc.:100.00%] [G loss:46.278282]\n",
      "881 [D loss: 0.010499, acc.:100.00%] [G loss:42.643433]\n",
      "882 [D loss: 0.005152, acc.:100.00%] [G loss:35.951401]\n",
      "883 [D loss: 0.002703, acc.:100.00%] [G loss:33.332527]\n",
      "884 [D loss: 0.004001, acc.:100.00%] [G loss:34.345425]\n",
      "885 [D loss: 0.131378, acc.:96.88%] [G loss:32.398575]\n",
      "886 [D loss: 0.001354, acc.:100.00%] [G loss:35.516247]\n",
      "887 [D loss: 0.005246, acc.:100.00%] [G loss:43.820618]\n",
      "888 [D loss: 0.054378, acc.:96.88%] [G loss:39.383774]\n",
      "889 [D loss: 0.009274, acc.:100.00%] [G loss:38.394859]\n",
      "890 [D loss: 0.015631, acc.:100.00%] [G loss:32.083595]\n",
      "891 [D loss: 0.016231, acc.:100.00%] [G loss:34.650970]\n",
      "892 [D loss: 0.022136, acc.:100.00%] [G loss:29.220570]\n",
      "893 [D loss: 0.003217, acc.:100.00%] [G loss:29.313343]\n",
      "894 [D loss: 0.001762, acc.:100.00%] [G loss:28.550299]\n",
      "895 [D loss: 0.003575, acc.:100.00%] [G loss:28.326313]\n",
      "896 [D loss: 0.001923, acc.:100.00%] [G loss:28.771978]\n",
      "897 [D loss: 0.001748, acc.:100.00%] [G loss:24.704220]\n",
      "898 [D loss: 0.002901, acc.:100.00%] [G loss:25.009502]\n",
      "899 [D loss: 0.004438, acc.:100.00%] [G loss:26.630318]\n",
      "900 [D loss: 0.004039, acc.:100.00%] [G loss:21.575325]\n",
      "901 [D loss: 0.026748, acc.:96.88%] [G loss:22.474560]\n",
      "902 [D loss: 0.001256, acc.:100.00%] [G loss:28.005089]\n",
      "903 [D loss: 0.001122, acc.:100.00%] [G loss:31.002470]\n",
      "904 [D loss: 0.008053, acc.:100.00%] [G loss:33.184441]\n",
      "905 [D loss: 0.011600, acc.:100.00%] [G loss:33.651093]\n",
      "906 [D loss: 0.007625, acc.:100.00%] [G loss:32.718437]\n",
      "907 [D loss: 0.073297, acc.:96.88%] [G loss:29.171183]\n",
      "908 [D loss: 0.007165, acc.:100.00%] [G loss:29.000206]\n",
      "909 [D loss: 0.001695, acc.:100.00%] [G loss:23.183517]\n",
      "910 [D loss: 0.001627, acc.:100.00%] [G loss:24.289253]\n",
      "911 [D loss: 0.003268, acc.:100.00%] [G loss:18.971191]\n",
      "912 [D loss: 0.155173, acc.:96.88%] [G loss:22.869469]\n",
      "913 [D loss: 0.002309, acc.:100.00%] [G loss:28.448441]\n",
      "914 [D loss: 0.008761, acc.:100.00%] [G loss:31.029003]\n",
      "915 [D loss: 0.030507, acc.:100.00%] [G loss:34.409164]\n",
      "916 [D loss: 0.030559, acc.:100.00%] [G loss:32.196041]\n",
      "917 [D loss: 0.006289, acc.:100.00%] [G loss:31.538401]\n",
      "918 [D loss: 0.004215, acc.:100.00%] [G loss:29.009871]\n",
      "919 [D loss: 0.008522, acc.:100.00%] [G loss:26.174957]\n",
      "920 [D loss: 0.009080, acc.:100.00%] [G loss:24.567003]\n",
      "921 [D loss: 0.006559, acc.:100.00%] [G loss:28.578876]\n",
      "922 [D loss: 0.005832, acc.:100.00%] [G loss:24.225929]\n",
      "923 [D loss: 0.003347, acc.:100.00%] [G loss:25.414612]\n",
      "924 [D loss: 0.006768, acc.:100.00%] [G loss:26.682186]\n",
      "925 [D loss: 0.003847, acc.:100.00%] [G loss:26.309113]\n",
      "926 [D loss: 0.010860, acc.:100.00%] [G loss:25.443443]\n",
      "927 [D loss: 0.001966, acc.:100.00%] [G loss:23.634556]\n",
      "928 [D loss: 0.002639, acc.:100.00%] [G loss:19.657299]\n",
      "929 [D loss: 0.002074, acc.:100.00%] [G loss:18.129108]\n",
      "930 [D loss: 0.003094, acc.:100.00%] [G loss:18.963688]\n",
      "931 [D loss: 0.001933, acc.:100.00%] [G loss:17.731224]\n",
      "932 [D loss: 0.001980, acc.:100.00%] [G loss:19.266182]\n",
      "933 [D loss: 0.001348, acc.:100.00%] [G loss:18.211292]\n",
      "934 [D loss: 0.001222, acc.:100.00%] [G loss:14.498933]\n",
      "935 [D loss: 0.001108, acc.:100.00%] [G loss:13.128214]\n",
      "936 [D loss: 0.007206, acc.:100.00%] [G loss:13.556400]\n",
      "937 [D loss: 0.002699, acc.:100.00%] [G loss:12.750534]\n",
      "938 [D loss: 0.001501, acc.:100.00%] [G loss:13.170603]\n",
      "939 [D loss: 0.010614, acc.:100.00%] [G loss:15.280499]\n",
      "940 [D loss: 0.002061, acc.:100.00%] [G loss:18.284389]\n",
      "941 [D loss: 0.030996, acc.:96.88%] [G loss:15.691399]\n",
      "942 [D loss: 0.002511, acc.:100.00%] [G loss:15.523592]\n",
      "943 [D loss: 0.008749, acc.:100.00%] [G loss:17.143911]\n",
      "944 [D loss: 0.002183, acc.:100.00%] [G loss:16.295494]\n",
      "945 [D loss: 0.003872, acc.:100.00%] [G loss:18.057571]\n",
      "946 [D loss: 0.002258, acc.:100.00%] [G loss:18.282749]\n",
      "947 [D loss: 0.003514, acc.:100.00%] [G loss:19.415718]\n",
      "948 [D loss: 0.004894, acc.:100.00%] [G loss:19.155052]\n",
      "949 [D loss: 0.010459, acc.:100.00%] [G loss:18.152933]\n",
      "950 [D loss: 0.003371, acc.:100.00%] [G loss:13.800595]\n",
      "951 [D loss: 0.001236, acc.:100.00%] [G loss:14.482002]\n",
      "952 [D loss: 0.005743, acc.:100.00%] [G loss:14.959675]\n",
      "953 [D loss: 0.001756, acc.:100.00%] [G loss:14.346725]\n",
      "954 [D loss: 0.002046, acc.:100.00%] [G loss:14.597332]\n",
      "955 [D loss: 0.002123, acc.:100.00%] [G loss:13.053772]\n",
      "956 [D loss: 0.001951, acc.:100.00%] [G loss:13.173201]\n",
      "957 [D loss: 0.011934, acc.:100.00%] [G loss:12.086535]\n",
      "958 [D loss: 0.002098, acc.:100.00%] [G loss:15.934876]\n",
      "959 [D loss: 0.006253, acc.:100.00%] [G loss:17.963942]\n",
      "960 [D loss: 0.002159, acc.:100.00%] [G loss:19.708698]\n",
      "961 [D loss: 0.004298, acc.:100.00%] [G loss:18.780693]\n",
      "962 [D loss: 0.004541, acc.:100.00%] [G loss:19.558178]\n",
      "963 [D loss: 0.003648, acc.:100.00%] [G loss:16.862547]\n",
      "964 [D loss: 0.006898, acc.:100.00%] [G loss:19.484818]\n",
      "965 [D loss: 0.003979, acc.:100.00%] [G loss:16.459949]\n",
      "966 [D loss: 0.002759, acc.:100.00%] [G loss:15.728477]\n",
      "967 [D loss: 0.003502, acc.:100.00%] [G loss:13.804585]\n",
      "968 [D loss: 0.003976, acc.:100.00%] [G loss:12.459267]\n",
      "969 [D loss: 0.003973, acc.:100.00%] [G loss:12.140285]\n",
      "970 [D loss: 0.001680, acc.:100.00%] [G loss:13.131287]\n",
      "971 [D loss: 0.001895, acc.:100.00%] [G loss:12.470122]\n",
      "972 [D loss: 0.003284, acc.:100.00%] [G loss:13.053979]\n",
      "973 [D loss: 0.003337, acc.:100.00%] [G loss:12.547544]\n",
      "974 [D loss: 0.002323, acc.:100.00%] [G loss:11.720682]\n",
      "975 [D loss: 0.006914, acc.:100.00%] [G loss:13.849842]\n",
      "976 [D loss: 0.001466, acc.:100.00%] [G loss:15.510065]\n",
      "977 [D loss: 0.002273, acc.:100.00%] [G loss:14.460926]\n",
      "978 [D loss: 0.002990, acc.:100.00%] [G loss:16.349979]\n",
      "979 [D loss: 0.013007, acc.:100.00%] [G loss:14.566830]\n",
      "980 [D loss: 0.003348, acc.:100.00%] [G loss:15.111841]\n",
      "981 [D loss: 0.002821, acc.:100.00%] [G loss:16.193575]\n",
      "982 [D loss: 0.002795, acc.:100.00%] [G loss:16.578604]\n",
      "983 [D loss: 0.001824, acc.:100.00%] [G loss:16.985975]\n",
      "984 [D loss: 0.001702, acc.:100.00%] [G loss:13.788861]\n",
      "985 [D loss: 0.001662, acc.:100.00%] [G loss:14.903087]\n",
      "986 [D loss: 0.002985, acc.:100.00%] [G loss:12.260775]\n",
      "987 [D loss: 0.002418, acc.:100.00%] [G loss:12.805899]\n",
      "988 [D loss: 0.002187, acc.:100.00%] [G loss:11.759916]\n",
      "989 [D loss: 0.002866, acc.:100.00%] [G loss:10.073849]\n",
      "990 [D loss: 0.001543, acc.:100.00%] [G loss:9.845078]\n",
      "991 [D loss: 0.001388, acc.:100.00%] [G loss:9.612198]\n",
      "992 [D loss: 0.004376, acc.:100.00%] [G loss:9.188267]\n",
      "993 [D loss: 0.001902, acc.:100.00%] [G loss:9.702909]\n",
      "994 [D loss: 0.051834, acc.:96.88%] [G loss:19.184900]\n",
      "995 [D loss: 0.005437, acc.:100.00%] [G loss:34.378464]\n",
      "996 [D loss: 0.088946, acc.:96.88%] [G loss:32.851116]\n",
      "997 [D loss: 0.009500, acc.:100.00%] [G loss:31.889551]\n",
      "998 [D loss: 0.005871, acc.:100.00%] [G loss:30.081181]\n",
      "999 [D loss: 0.024815, acc.:96.88%] [G loss:38.141579]\n"
     ]
    }
   ],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "generator = build_generator()\n",
    "generator.compile(loss='binary_crossentropy', optimizer = 'adam')\n",
    "\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# Freeze discrimnator while we train generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "valid = discriminator(img)\n",
    "\n",
    "combined = Model(z, valid)\n",
    "combined.compile(loss='binary_crossentropy', optimizer = 'adam')\n",
    "\n",
    "train(epochs =1000, batch_size =32, save_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef68740",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('generator_model_test.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a77f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
